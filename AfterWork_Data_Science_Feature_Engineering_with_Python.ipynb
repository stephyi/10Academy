{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AfterWork Data Science: Feature Engineering with Python",
      "provenance": [],
      "collapsed_sections": [
        "bui-wArzDV7y",
        "7tKTkqsqk7Pe",
        "bEC2w3co7eUG",
        "ieW4rT26uUcL",
        "QmLWHKAmeSq1",
        "2QSb-k93uQLZ",
        "5UjUUJgY7eUJ",
        "7LKbMFnX5WTb",
        "kNTbrgG4lOly",
        "-nOFP4kz794p",
        "DhpcFqQ5794y",
        "8XADQeRZ7946",
        "hK70cfMA7947",
        "d5pnSWF7UZgu",
        "mgaEmj1KUZg3",
        "W9MJ2NRmUZhA",
        "e99bEufxUZhB",
        "az28_PBIy7cp",
        "jIY_ZmuXy7cx",
        "5bDPQCL9y7c9",
        "VurrOYNQy7c9",
        "Qo_3QQqoJLCU",
        "fkA7TmGZJLCc",
        "_9sozKhlJLCj",
        "_L-STcjlJLCj",
        "kOhmc9kIQbeO",
        "OIRnh9JaQbeb",
        "a-8bcJFNQbei",
        "U23W7b4EQbei",
        "1nmgDesxbvui",
        "8TXmKueXflMV"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stephyi/10Academy/blob/master/AfterWork_Data_Science_Feature_Engineering_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igAqIIO5gmvV",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"blue\">To use this notebook on Google Colaboratory, you will need to make a copy of it. Go to **File** > **Save a Copy in Drive**. You can then use the new copy that will appear in the new tab.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyO7-7S3gpAU",
        "colab_type": "text"
      },
      "source": [
        "# AfterWork Data Science: Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bui-wArzDV7y",
        "colab_type": "text"
      },
      "source": [
        "### Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6ThCsEAXaeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's first import the libraries that we will need\n",
        "# ----\n",
        "#\n",
        "import pandas as pd               # pandas for performing data manipulation\n",
        "import numpy as np                # numpy for performing scientific computations\n",
        "import matplotlib.pyplot as plt   # matplotlib for performing visualisation "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tKTkqsqk7Pe",
        "colab_type": "text"
      },
      "source": [
        "## Feature Improvement Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bEC2w3co7eUG"
      },
      "source": [
        "#### <font color=\"blue\">Example: Standardisation & Normalisation</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "10VjGvv-7eUH",
        "colab": {}
      },
      "source": [
        "# Example\n",
        "# --- \n",
        "# Question: Using the Support Vector Regressor, create a regression model using the clean dataset below.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/FishDatasetClean\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieW4rT26uUcL",
        "colab_type": "text"
      },
      "source": [
        "##### Step 1. Loading our Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh0gAyrircP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading our dataset\n",
        "# ---\n",
        "# \n",
        "df = pd.read_csv('http://bit.ly/FishDatasetClean')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KROjf7vN27c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting a statistical summary of our dataset\n",
        "# ---\n",
        "# \n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmLWHKAmeSq1",
        "colab_type": "text"
      },
      "source": [
        "##### Step 2, 3, 4: Checking, Cleaning, Exploratory Analysis and have already been performed on our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QSb-k93uQLZ",
        "colab_type": "text"
      },
      "source": [
        "##### Step 5. Implementation and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk21ZzxL1NMK",
        "colab_type": "text"
      },
      "source": [
        "We will now perform normalisation and standardisation techniques to our dataset then fit the data to various models. We can then compare out RMSE accuracy in different instances. Go through each of the given models to understand the effect on these two techniques. Remember to uncomment the relevant cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af0jSgXfs8O2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First we check for modeling without without normalisation and standardisation\n",
        "# ---\n",
        "\n",
        "# We select our features\n",
        "X = df[['Length1', 'Length2', 'Length3', 'Height', 'Width']]\n",
        "y = df['Weight']\n",
        "\n",
        "# Splitting our dataset \n",
        "# ---\n",
        "# NB: We use random_state to get the same results everytime,\n",
        "# else we'd get to be working with different test and train datasets.\n",
        "# ---\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state = 42) \n",
        "\n",
        "# Fitting in our models \n",
        "# ---\n",
        "from sklearn.svm import SVR \n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor  \n",
        "\n",
        "# Don't worry about the model parameters, we will learn about \n",
        "# them in a separate workshop\n",
        "svm_regressor = SVR(kernel='rbf', C=10)\n",
        "knn_regressor = KNeighborsRegressor()\n",
        "dec_regressor = DecisionTreeRegressor(random_state=27)\n",
        "\n",
        "svm_regressor.fit(X_train, y_train)\n",
        "knn_regressor.fit(X_train, y_train)\n",
        "dec_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Making Predictions  \n",
        "svm_y_pred = svm_regressor.predict(X_test)\n",
        "knn_y_pred = knn_regressor.predict(X_test)\n",
        "dec_y_pred = dec_regressor.predict(X_test)\n",
        "\n",
        "# Finally, evaluate our model \n",
        "from sklearn import metrics \n",
        "print('SVM RMSE:', np.sqrt(metrics.mean_squared_error(y_test, svm_y_pred)))\n",
        "print('KNN RMSE:', np.sqrt(metrics.mean_squared_error(y_test, knn_y_pred)))\n",
        "print('Decision Tree RMSE:', np.sqrt(metrics.mean_squared_error(y_test, dec_y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m81fa6aqr1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We then check for modeling with only normalisation\n",
        "# ---\n",
        "\n",
        "# We select our features\n",
        "X = df[['Length1', 'Length2', 'Length3', 'Height', 'Width']]\n",
        "y = df['Weight']\n",
        "\n",
        "# Splitting our dataset  \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state = 42)\n",
        "\n",
        "# Performing normalisation \n",
        "norm = MinMaxScaler().fit(X_train) \n",
        "X_train = norm.transform(X_train) \n",
        "X_test = norm.transform(X_test)\n",
        "\n",
        "# Fitting in our models  \n",
        "svm_regressor = SVR(kernel='rbf', C=10)\n",
        "knn_regressor = KNeighborsRegressor()\n",
        "dec_regressor = DecisionTreeRegressor(random_state=27)\n",
        "\n",
        "svm_regressor.fit(X_train, y_train)\n",
        "knn_regressor.fit(X_train, y_train)\n",
        "dec_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Making Predictions  \n",
        "svm_y_pred = svm_regressor.predict(X_test)\n",
        "knn_y_pred = knn_regressor.predict(X_test)\n",
        "dec_y_pred = dec_regressor.predict(X_test)\n",
        "\n",
        "# Finally, evaluating our models \n",
        "print('SVM RMSE:', np.sqrt(metrics.mean_squared_error(y_test, svm_y_pred)))\n",
        "print('KNN RMSE:', np.sqrt(metrics.mean_squared_error(y_test, knn_y_pred)))\n",
        "print('Decision Tree RMSE:', np.sqrt(metrics.mean_squared_error(y_test, dec_y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlBbOFQJoKwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First we check for modeling with standardisation\n",
        "# ---\n",
        "\n",
        "# We select our features\n",
        "X = df[['Length1', 'Length2', 'Length3', 'Height', 'Width']]\n",
        "y = df['Weight']\n",
        "\n",
        "# Splitting our dataset  \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state = 42)\n",
        "\n",
        "# Performing standardisation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler() \n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.fit_transform(X_test)\n",
        "\n",
        "# Fitting in our models  \n",
        "svm_regressor = SVR(kernel='rbf', C=10)\n",
        "knn_regressor = KNeighborsRegressor()\n",
        "dec_regressor = DecisionTreeRegressor(random_state=27)\n",
        "\n",
        "svm_regressor.fit(X_train, y_train)\n",
        "knn_regressor.fit(X_train, y_train)\n",
        "dec_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Making Predictions  \n",
        "svm_y_pred = svm_regressor.predict(X_test)\n",
        "knn_y_pred = knn_regressor.predict(X_test)\n",
        "dec_y_pred = dec_regressor.predict(X_test)\n",
        "\n",
        "# Finally, evaluating our models \n",
        "print('SVM RMSE:', np.sqrt(metrics.mean_squared_error(y_test, svm_y_pred)))\n",
        "print('KNN RMSE:', np.sqrt(metrics.mean_squared_error(y_test, knn_y_pred)))\n",
        "print('Decision Tree RMSE:', np.sqrt(metrics.mean_squared_error(y_test, dec_y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5UjUUJgY7eUJ"
      },
      "source": [
        "#### <font color=\"green\">Challenge</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ljuhnH0k7eUL",
        "colab": {}
      },
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# You can now work on the following dataset that you've used in the past to apply \n",
        "# the scaling techniques in an effort improve accuracy.\n",
        "# Create a regression model to predict price using the given dataset examining\n",
        "# the two scaling techniques for the different regressors. \n",
        "# NB: You can apply the other regression technique that we did not use i.e.\n",
        "# Multiple linear regression.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/RealEstateDataset2\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LKbMFnX5WTb",
        "colab_type": "text"
      },
      "source": [
        "#### <font color=\"green\">Challenge 2</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeP6EWAJ5bMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Challenge 2\n",
        "# ---\n",
        "# Again, you've already gone this classification problem. \n",
        "# Build a classifier to predict car sales, check the accuracy of the prediction then challenge \n",
        "# your solution by apply feature improvement techniques to  following dataset\n",
        "# ---\n",
        "# Dataset url = https://bit.ly/3dvU2BB\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNTbrgG4lOly",
        "colab_type": "text"
      },
      "source": [
        "## Feature Selection Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-nOFP4kz794p"
      },
      "source": [
        "#### Filter Method: <font color=\"blue\">Example: Pearson's Correlation Coefficient</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UrchhMZc794s",
        "colab": {}
      },
      "source": [
        "# Example\n",
        "# --- \n",
        "# Question: Let's use the following dataset that we used above.\n",
        "# We will use the pearson's correlation coefficient as our filtering method.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/FishDatasetClean\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DhpcFqQ5794y"
      },
      "source": [
        "##### Step 1. Loading our Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ip2-u1W7794y",
        "colab": {}
      },
      "source": [
        "# Loading our dataset\n",
        "# ---\n",
        "# \n",
        "df = pd.read_csv('http://bit.ly/FishDatasetClean')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HA3qiZOA7942",
        "colab": {}
      },
      "source": [
        "# Describing our dataset\n",
        "# ---\n",
        "# \n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8XADQeRZ7946"
      },
      "source": [
        "##### Step 2, 3, 4: Checking, Cleaning, Exploratory Analysis and have already been performed on our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hK70cfMA7947"
      },
      "source": [
        "##### Step 5. Implementation and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Icq79FHU7947"
      },
      "source": [
        "In this example we will only use the pearson's correlation coefficient to resolve the most important features in a dataset. In our case we will drop features that are not highly correlated to our response variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P5lSLbNx795F",
        "colab": {}
      },
      "source": [
        "# First, we then perform modeling with both standardisation and normalisation.\n",
        "# We will use this as as our base for our solution, then perform feature engineering \n",
        "# by filter methods.\n",
        "# ---\n",
        "\n",
        "# We select our features\n",
        "X = df[['Length1', 'Length2', 'Length3', 'Height', 'Width']]\n",
        "y = df['Weight']\n",
        "\n",
        "# Splitting our dataset  \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state = 42)\n",
        "\n",
        "# Performing normalisation \n",
        "norm = MinMaxScaler().fit(X_train) \n",
        "X_train = norm.transform(X_train) \n",
        "X_test = norm.transform(X_test)\n",
        "\n",
        "# Fitting in our models   \n",
        "svm_regressor = SVR(kernel='rbf', C=10)\n",
        "knn_regressor = KNeighborsRegressor()\n",
        "dec_regressor = DecisionTreeRegressor(random_state=27)\n",
        "\n",
        "svm_regressor.fit(X_train, y_train)\n",
        "knn_regressor.fit(X_train, y_train)\n",
        "dec_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Making Predictions  \n",
        "svm_y_pred = svm_regressor.predict(X_test)\n",
        "knn_y_pred = knn_regressor.predict(X_test)\n",
        "dec_y_pred = dec_regressor.predict(X_test)\n",
        "\n",
        "# Finally, evaluating our models  \n",
        "print('SVM RMSE:', np.sqrt(metrics.mean_squared_error(y_test, svm_y_pred)))\n",
        "print('KNN RMSE:', np.sqrt(metrics.mean_squared_error(y_test, knn_y_pred)))\n",
        "print('Decision Tree RMSE:', np.sqrt(metrics.mean_squared_error(y_test, dec_y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgi2aiO5lRMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Then appply filter methods by plotting a correlation matrix\n",
        "# ---\n",
        "#\n",
        "df_corr = df.corr()\n",
        "plt.figure(figsize=(5,4))\n",
        "\n",
        "# We then plot our heatmap visualistion\n",
        "# \n",
        "import seaborn as sns\n",
        "sns.heatmap(df_corr, annot=True, linewidth=0.5, cmap='coolwarm');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR0cRPpG_7mh",
        "colab_type": "text"
      },
      "source": [
        "We resolve to drop height since it has a weaker correlation to Weight, which is our response variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gShh2ZSzAFPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Then perform our modeling, comparing the resulting accuracy to the previous base solution.\n",
        "# ---\n",
        "# We select our features\n",
        "X = df[['Length1', 'Length2', 'Length3', 'Width']]\n",
        "y = df['Weight']\n",
        "\n",
        "# Splitting our dataset  \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state = 42)\n",
        "\n",
        "# Performing normalisation \n",
        "norm = MinMaxScaler().fit(X_train) \n",
        "X_train = norm.transform(X_train) \n",
        "X_test = norm.transform(X_test)\n",
        "\n",
        "# Fitting in our models   \n",
        "svm_regressor = SVR(kernel='rbf', C=10)\n",
        "knn_regressor = KNeighborsRegressor()\n",
        "dec_regressor = DecisionTreeRegressor(random_state=27)\n",
        "\n",
        "svm_regressor.fit(X_train, y_train)\n",
        "knn_regressor.fit(X_train, y_train)\n",
        "dec_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Making Predictions  \n",
        "svm_y_pred = svm_regressor.predict(X_test)\n",
        "knn_y_pred = knn_regressor.predict(X_test)\n",
        "dec_y_pred = dec_regressor.predict(X_test)\n",
        "\n",
        "# Finally, evaluate our model \n",
        "print('SVM RMSE:', np.sqrt(metrics.mean_squared_error(y_test, svm_y_pred)))\n",
        "print('KNN RMSE:', np.sqrt(metrics.mean_squared_error(y_test, knn_y_pred)))\n",
        "print('Decision Tree RMSE:', np.sqrt(metrics.mean_squared_error(y_test, dec_y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d5pnSWF7UZgu"
      },
      "source": [
        "#### Wrapper Method: <font color=\"blue\">Example: Step Forward Feature Selection</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sxnGdeABUZgw",
        "colab": {}
      },
      "source": [
        "# Example\n",
        "# --- \n",
        "# Question: Let's use the following dataset that we used above in our example \n",
        "# and perform the step forward feature selection method.\n",
        "# --\n",
        "# During step forward feature selection you start with no variables in the model, \n",
        "# testing the addition of each variable using a chosen model fit criterion, \n",
        "# adding the variable (if any) whose inclusion gives the most statistically \n",
        "# significant improvement of the fit, and repeating this process until none \n",
        "# improves the model to a statistically significant extent.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/FishDatasetClean\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mgaEmj1KUZg3"
      },
      "source": [
        "##### Step 1. Loading our Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FErFyyYmUZg4",
        "colab": {}
      },
      "source": [
        "# Loading our dataset\n",
        "# ---\n",
        "# \n",
        "df = pd.read_csv('http://bit.ly/FishDatasetClean')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vS3GAvqwUZg9",
        "colab": {}
      },
      "source": [
        "# Describing our dataset\n",
        "# ---\n",
        "# \n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W9MJ2NRmUZhA"
      },
      "source": [
        "##### Step 2, 3, 4: Checking, Cleaning, Exploratory Analysis and have already been performed on our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e99bEufxUZhB"
      },
      "source": [
        "##### Step 5. Implementation and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hmGJkG5dUZhC",
        "colab": {}
      },
      "source": [
        "# First, we then perform modeling with both standardisation and normalisation.\n",
        "# We will use this as our base for our solution, then perform feature engineering \n",
        "# by filter methods.\n",
        "# ---\n",
        "\n",
        "# We select our features\n",
        "X = df[['Length1', 'Length2', 'Length3', 'Height', 'Width']]\n",
        "y = df['Weight']\n",
        "\n",
        "# Splitting our dataset  \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state = 42)\n",
        "\n",
        "# Performing normalisation \n",
        "norm = MinMaxScaler().fit(X_train) \n",
        "X_train = norm.transform(X_train) \n",
        "X_test = norm.transform(X_test)\n",
        "\n",
        "# Selecting the ML algorithm to use   \n",
        "dec_regressor = DecisionTreeRegressor(random_state=27)\n",
        "\n",
        "# We pass the svm_regressor the estimator to the SequentialFeatureSelector function. \n",
        "# The k_features specifies the number of features to select. \n",
        "# We can set any number of features here. The forward parameter, if set to True, \n",
        "# performs step forward feature selection. The verbose parameter is used for logging \n",
        "# the progress of the feature selector, the scoring parameter defines the performance \n",
        "# evaluation criteria and finally, cv refers to cross-validation folds.\n",
        "# ---\n",
        "# Hint: Hover cursor on SequentialFeatureSelector to get a list of more parameter values.\n",
        "# ---\n",
        "#\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector\n",
        "feature_selector = SequentialFeatureSelector(dec_regressor,\n",
        "           k_features=4,\n",
        "           forward=True,\n",
        "           verbose=2,\n",
        "           scoring='r2',\n",
        "           cv=4)\n",
        " \n",
        "# Perform step forward feature selection\n",
        "feature_selector = feature_selector.fit(X_train, y_train) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umGBjww2gmkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Which are the selected features?\n",
        "# The columns at these indexes are those which were selected\n",
        "# ---\n",
        "#\n",
        "feat_cols = list(feature_selector.k_feature_idx_)\n",
        "print(feat_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DzHw8MygZy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can now use those features to build our model\n",
        "# ---\n",
        "# \n",
        "\n",
        "# Without step forward feature selection (sffs)\n",
        "dec_regressor = DecisionTreeRegressor(random_state=27)\n",
        "dec_regressor.fit(X_train, y_train)\n",
        "\n",
        "# With step forward feature selection\n",
        "dec_regressor2 = DecisionTreeRegressor(random_state=27)\n",
        "dec_regressor2.fit(X_train[:, feat_cols], y_train)\n",
        "\n",
        "# Making Predictions and determining the accuracies\n",
        "y_test_pred = dec_regressor.predict(X_test)\n",
        "print('Decision Tree RMSE Without sffs:', np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))\n",
        "\n",
        "y_test_pred2 = dec_regressor2.predict(X_test[:, feat_cols])\n",
        "print('Decision Tree RMSE with sffs:', np.sqrt(metrics.mean_squared_error(y_test, y_test_pred2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "az28_PBIy7cp"
      },
      "source": [
        "#### Wrapper Method: <font color=\"blue\">Example: Step Backward Feature Selection</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ajJPfDFVy7ct",
        "colab": {}
      },
      "source": [
        "# Example\n",
        "# --- \n",
        "# Question: Let's use the following dataset that we used above in our example \n",
        "# and perform the step backward feature selection method.\n",
        "# --\n",
        "# Step backward feature selection involves starting with all candidate variables, \n",
        "# testing the deletion of each variable using a chosen model fit criterion, \n",
        "# deleting the variable (if any) whose loss gives the most statistically \n",
        "# insignificant deterioration of the model fit, \n",
        "# and repeating this process until no further variables can be deleted without \n",
        "# a statistically insignificant loss of fit.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/FishDatasetClean\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jIY_ZmuXy7cx"
      },
      "source": [
        "##### Step 1. Loading our Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F8X7jwEfy7cy",
        "colab": {}
      },
      "source": [
        "# Loading our dataset\n",
        "# ---\n",
        "# \n",
        "df = pd.read_csv('http://bit.ly/FishDatasetClean')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OUrUk7kPy7c2",
        "colab": {}
      },
      "source": [
        "# Describing our dataset\n",
        "# ---\n",
        "# \n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5bDPQCL9y7c9"
      },
      "source": [
        "##### Step 2, 3, 4: Checking, Cleaning, Exploratory Analysis and have already been performed on our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VurrOYNQy7c9"
      },
      "source": [
        "##### Step 5. Implementation and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "46cdpZFGy7c-",
        "colab": {}
      },
      "source": [
        "# First, we then perform modeling with both standardisation and normalisation.\n",
        "# We will use this as our base for our solution, then perform feature engineering \n",
        "# by filter methods.\n",
        "# ---\n",
        "\n",
        "# We select our features\n",
        "X = df[['Length1', 'Length2', 'Length3', 'Height', 'Width']]\n",
        "y = df['Weight']\n",
        "\n",
        "# Splitting our dataset  \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state = 42)\n",
        "\n",
        "# Performing normalisation \n",
        "norm = MinMaxScaler().fit(X_train) \n",
        "X_train = norm.transform(X_train) \n",
        "X_test = norm.transform(X_test)\n",
        "\n",
        "# Selecting the ML algorithm to use   \n",
        "dec_regressor = DecisionTreeRegressor(random_state=27)\n",
        "\n",
        "# We pass the dec_regressor the estimator to the SequentialFeatureSelector function. \n",
        "# The k_features specifies the number of features to select. \n",
        "# We can set any number of features here. The forward parameter, if set to False, \n",
        "# performs step backward feature selection. The verbose parameter is used for logging \n",
        "# the progress of the feature selector, the scoring parameter defines the performance \n",
        "# evaluation criteria and finally, cv refers to cross-validation folds.\n",
        "# ---\n",
        "# Hint: Hover cursor on SequentialFeatureSelector to get a list of more parameter values.\n",
        "# ---\n",
        "#\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector\n",
        "feature_selector = SequentialFeatureSelector(dec_regressor,\n",
        "           k_features=4,\n",
        "           forward=False,\n",
        "           verbose=2,\n",
        "           scoring='r2',\n",
        "           cv=4)\n",
        " \n",
        "# Perform step backward feature selection\n",
        "feature_selector = feature_selector.fit(X_train, y_train) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_LeH-JWxy7dB",
        "colab": {}
      },
      "source": [
        "# Which are the selected features?\n",
        "# The columns at these indexes are those which were selected\n",
        "# ---\n",
        "#\n",
        "feat_cols = list(feature_selector.k_feature_idx_)\n",
        "print(feat_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A2Zd98lsy7dE",
        "colab": {}
      },
      "source": [
        "# We can now use those features to build a full model\n",
        "# ---\n",
        "# \n",
        "\n",
        "# Without step backward feature selection (sbfs)\n",
        "dec_regressor = DecisionTreeRegressor(random_state=27)\n",
        "dec_regressor.fit(X_train, y_train)\n",
        "\n",
        "# With step backward feature selection\n",
        "dec_regressor2 = DecisionTreeRegressor(random_state=27)\n",
        "dec_regressor2.fit(X_train[:, feat_cols], y_train)\n",
        "\n",
        "# Making Predictions and determining the accuracies  \n",
        "y_test_pred = dec_regressor.predict(X_test)\n",
        "print('Decision Tree RMSE Without sbfs:', np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))\n",
        "\n",
        "y_test_pred2 = dec_regressor2.predict(X_test[:, feat_cols])\n",
        "print('Decision Tree RMSE with sbfs:', np.sqrt(metrics.mean_squared_error(y_test, y_test_pred2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qo_3QQqoJLCU"
      },
      "source": [
        "#### Wrapper Method: <font color=\"blue\">Example: Recursive Feature Elimination</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "67BYWejtJLCW",
        "colab": {}
      },
      "source": [
        "# Example\n",
        "# --- \n",
        "# Question: Let's use the following dataset that we used above in our example to\n",
        "# use recursive feature elimination as our filtering method.\n",
        "# --\n",
        "# The Recursive Feature Elimination (RFE) method is a feature selection approach which \n",
        "# works by recursively removing attributes and building a model on those attributes that remain. \n",
        "# It uses the model accuracy to identify which attributes (and combination of attributes) \n",
        "# contribute the most to predicting the target attribute.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/FishDatasetClean\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fkA7TmGZJLCc"
      },
      "source": [
        "##### Step 1. Loading our Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YQ8cpR0UJLCc",
        "colab": {}
      },
      "source": [
        "# Loading our dataset\n",
        "# ---\n",
        "# \n",
        "df = pd.read_csv('http://bit.ly/FishDatasetClean')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NMQRNg0NJLCg",
        "colab": {}
      },
      "source": [
        "# Describing our dataset\n",
        "# ---\n",
        "# \n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_9sozKhlJLCj"
      },
      "source": [
        "##### Step 2, 3, 4: Checking, Cleaning, Exploratory Analysis and have already been performed on our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_L-STcjlJLCj"
      },
      "source": [
        "##### Step 5. Implementation and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HBBzuWBGJLCk",
        "colab": {}
      },
      "source": [
        "# First, we then perform modeling with both standardisation and normalisation.\n",
        "# We will use this as our base for our solution, then perform feature engineering \n",
        "# by filter methods.\n",
        "# ---\n",
        "\n",
        "# We select our features\n",
        "X = df[['Length1', 'Length2', 'Length3', 'Height', 'Width']]\n",
        "y = df['Weight']\n",
        "\n",
        "# Splitting our dataset  \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state = 42)\n",
        " \n",
        "# Performing normalisation \n",
        "norm = MinMaxScaler().fit(X_train) \n",
        "X_train = norm.transform(X_train) \n",
        "X_test = norm.transform(X_test)\n",
        "\n",
        "# Fitting in our models   \n",
        "\n",
        "svm_regressor = SVR(kernel=\"linear\")   \n",
        "dec_regressor = DecisionTreeRegressor(random_state=27)\n",
        "\n",
        "# We want to select the best 2 features for our model. \n",
        "# NB: n_features_to_select will include the response variable\n",
        "# ---\n",
        "#  \n",
        "from sklearn.feature_selection import RFE\n",
        "svm_regressor = RFE(svm_regressor, n_features_to_select = 3, step=1)\n",
        "dec_regressor = RFE(dec_regressor, n_features_to_select = 3, step=1)\n",
        "\n",
        "svm_regressor.fit(X_train, y_train) \n",
        "dec_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Making Predictions  \n",
        "svm_y_pred = svm_regressor.predict(X_test) \n",
        "dec_y_pred = dec_regressor.predict(X_test)\n",
        "\n",
        "# Finally, evaluate our model  \n",
        "print('SVM RMSE:', np.sqrt(metrics.mean_squared_error(y_test, svm_y_pred))) \n",
        "print('Decision Tree RMSE:', np.sqrt(metrics.mean_squared_error(y_test, dec_y_pred)))\n",
        " \n",
        "# Displaying our best features\n",
        "print('SVM Selected features: %s' % list(X.columns[svm_regressor.support_]))\n",
        "print('Decision Tree Selected features: %s' % list(X.columns[dec_regressor.support_]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kOhmc9kIQbeO"
      },
      "source": [
        "#### Feature Transformation: <font color=\"blue\">Example: Principal Component Analysis</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2QtlyumlQbeV",
        "colab": {}
      },
      "source": [
        "# Example\n",
        "# --- \n",
        "# Question: Let's use the following dataset that we used above in our example to\n",
        "# use the principal component analysis (PCA) to reduce our features into components.\n",
        "# ---\n",
        "\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/FishDatasetClean\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OIRnh9JaQbeb"
      },
      "source": [
        "##### Step 1. Loading our Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uNX4f4a8Qbec",
        "colab": {}
      },
      "source": [
        "# Loading our dataset\n",
        "# ---\n",
        "# \n",
        "df = pd.read_csv('http://bit.ly/FishDatasetClean')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9CwJEyCMQbef",
        "colab": {}
      },
      "source": [
        "# Describing our dataset\n",
        "# ---\n",
        "# \n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a-8bcJFNQbei"
      },
      "source": [
        "##### Step 2, 3, 4: Checking, Cleaning, Exploratory Analysis and have already been performed on our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U23W7b4EQbei"
      },
      "source": [
        "##### Step 5. Implementation and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiAD9CaqRAyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Again, create our base models and check for the accuracy to later compare it\n",
        "# later with our PCA implementation.\n",
        "# ---\n",
        "\n",
        "# We select our features\n",
        "X = df[['Length1', 'Length2', 'Length3', 'Height', 'Width']]\n",
        "y = df['Weight']\n",
        "\n",
        "# Splitting our dataset  \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state = 42)\n",
        "\n",
        "# Performing normalisation  \n",
        "norm = MinMaxScaler().fit(X_train) \n",
        "X_train = norm.transform(X_train) \n",
        "X_test = norm.transform(X_test)\n",
        "\n",
        "# Fitting in our models   \n",
        "svm_regressor = SVR(kernel='rbf', C=10)\n",
        "knn_regressor = KNeighborsRegressor()\n",
        "dec_regressor = DecisionTreeRegressor(random_state=27)\n",
        "\n",
        "svm_regressor.fit(X_train, y_train)\n",
        "knn_regressor.fit(X_train, y_train)\n",
        "dec_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Making Predictions  \n",
        "svm_y_pred = svm_regressor.predict(X_test)\n",
        "knn_y_pred = knn_regressor.predict(X_test)\n",
        "dec_y_pred = dec_regressor.predict(X_test)\n",
        "\n",
        "# Finally, evaluating our models  \n",
        "print('SVM RMSE:', np.sqrt(metrics.mean_squared_error(y_test, svm_y_pred)))\n",
        "print('KNN RMSE:', np.sqrt(metrics.mean_squared_error(y_test, knn_y_pred)))\n",
        "print('Decision Tree RMSE:', np.sqrt(metrics.mean_squared_error(y_test, dec_y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61Ra1qxyROno",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Again, then apply PCA to our dataset\n",
        "# ---\n",
        "\n",
        "# We select our features\n",
        "X = df[['Length1', 'Length2', 'Length3', 'Height', 'Width']]\n",
        "y = df['Weight']\n",
        "\n",
        "# Splitting our dataset  \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state = 42)\n",
        "\n",
        "# Performing normalisation \n",
        "norm = MinMaxScaler().fit(X_train) \n",
        "X_train = norm.transform(X_train) \n",
        "X_test = norm.transform(X_test)\n",
        "\n",
        "# Applying PCA\n",
        "# ---\n",
        "# NB: PCA relies the feature set and not the label data.\n",
        "# ---\n",
        "# \n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "# Fitting in our models   \n",
        "svm_regressor = SVR(kernel='rbf', C=10)\n",
        "knn_regressor = KNeighborsRegressor()\n",
        "dec_regressor = DecisionTreeRegressor(random_state=27)\n",
        "\n",
        "svm_regressor.fit(X_train, y_train)\n",
        "knn_regressor.fit(X_train, y_train)\n",
        "dec_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Making Predictions  \n",
        "svm_y_pred = svm_regressor.predict(X_test)\n",
        "knn_y_pred = knn_regressor.predict(X_test)\n",
        "dec_y_pred = dec_regressor.predict(X_test)\n",
        "\n",
        "# Finally, evaluating our models \n",
        "print('SVM RMSE:', np.sqrt(metrics.mean_squared_error(y_test, svm_y_pred)))\n",
        "print('KNN RMSE:', np.sqrt(metrics.mean_squared_error(y_test, knn_y_pred)))\n",
        "print('Decision Tree RMSE:', np.sqrt(metrics.mean_squared_error(y_test, dec_y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1nmgDesxbvui"
      },
      "source": [
        "#### <font color=\"green\">Challenge 1</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tMPuv73cbvuj",
        "colab": {}
      },
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# Perform the above feature selection techniques to improve the accuracy of \n",
        "# your model that you use to predict prices in the previously used real estate dataset.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/RealEstateDataset2\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8TXmKueXflMV"
      },
      "source": [
        "#### Feature Transformation: <font color=\"green\">Challenge: Linear Discriminant Analysis</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OUYkkWCjflMY",
        "colab": {}
      },
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# From day 1, we have held your hand in providing for examples that you'd learn from, \n",
        "# in order to work on the challenges. This time we would like you to refer to the \n",
        "# LDA sklearn documentation (Google) and then later perform LDA on the following dataset \n",
        "# with the main goal of improving the accuracy of your model.   \n",
        "# ---\n",
        "# Dataset url = http://bit.ly/2So1eGk\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}